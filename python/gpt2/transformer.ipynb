{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import typing\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttentionHeadConfig:\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: AttentionHeadConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # linear layers to project the input to the key, query and value vectors\n",
    "\n",
    "        self.q = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        self.k = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        self.v = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.ctx_len, config.ctx_len)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        # q in (batch_size, ctx_len, d_k)\n",
    "        q = self.q(x)\n",
    "        # k in (batch_size, ctx_len, d_k)\n",
    "        k = self.k(x)\n",
    "\n",
    "        # masked self attention\n",
    "        # a in (batch_size, ctx_len, ctx_len)\n",
    "        a = (q @ k.transpose(-2, -1)) / (self.config.d_k ** 0.5)\n",
    "        a = a.masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "        a = F.softmax(a, dim=-1)\n",
    "        \n",
    "        # v in (batch_size, ctx_len, d_k)\n",
    "        v = self.v(x)\n",
    "        \n",
    "        # att in (batch_size, ctx_len, d_k)        \n",
    "        att = a @ v\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultiHeadAttentionConfig:\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Dimension of the output vector\n",
    "    d_out: int\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: MultiHeadAttentionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(AttentionHeadConfig(\n",
    "                d_embed=config.d_embed,\n",
    "                d_k=config.d_k,\n",
    "                ctx_len=config.ctx_len\n",
    "            )) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.o = nn.Linear(config.n_heads*config.d_k, config.d_out, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.o(torch.cat([head(x) for head in self.heads], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerBlockConfig:\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Width of the feed-forward network\n",
    "    ff_width: int\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: TransformerBlockConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.ln1 = nn.LayerNorm(config.d_embed)\n",
    "        self.attn = MultiHeadAttention(MultiHeadAttentionConfig(\n",
    "            d_embed=config.d_embed,\n",
    "            d_k=config.d_k,\n",
    "            ctx_len=config.ctx_len,\n",
    "            n_heads=config.n_heads,\n",
    "            d_out=config.d_embed\n",
    "        ))\n",
    "        self.ln2 = nn.LayerNorm(config.d_embed)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.d_embed, config.ff_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.ff_width, config.d_embed)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Width of the feed-forward network\n",
    "    ff_width: int\n",
    "    # Number of transformer blocks\n",
    "    n_blocks: int\n",
    "    # Number of tokens in the vocabulary\n",
    "    vocab_size: int\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_embed)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(TransformerBlockConfig(\n",
    "                d_embed=config.d_embed,\n",
    "                d_k=config.d_k,\n",
    "                ctx_len=config.ctx_len,\n",
    "                n_heads=config.n_heads,\n",
    "                ff_width=config.ff_width\n",
    "            )) for _ in range(config.n_blocks)\n",
    "        ])\n",
    "        self.unembed = nn.Linear(config.d_embed, config.vocab_size)\n",
    "\n",
    "    # one of the reasons that transformers are so popular is that they give you ctx_len tokens in parallel\n",
    "    # they predict the probability distribution over the next token for each of the ctx_len tokens\n",
    "    # this is why the input is (batch_size, ctx_len) and the output is (batch_size, ctx_len, vocab_size)\n",
    "    def forward(self, x):\n",
    "        # x in (batch_size, ctx_len)\n",
    "        x = self.embed(x)\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        x = self.unembed(x)\n",
    "        # x in (batch_size, ctx_len, vocab_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{} \\n\\t\"\n",
    "vocab = defaultdict(lambda: 1)\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<unk>\"] = 1\n",
    "for i, c in enumerate(chars):\n",
    "    vocab[c] = i + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text:str) -> torch.Tensor:\n",
    "    return torch.tensor([vocab[c] for c in text], dtype=torch.long)\n",
    "\n",
    "\n",
    "def prep_training_data(dataset:list[str], ctx_len):\n",
    "    # tokenize each data source:\n",
    "    X = []\n",
    "    Y = []\n",
    "    for text in dataset:\n",
    "        # tokenize\n",
    "        tokenized_text = tokenize(text)\n",
    "        if len(tokenized_text) > 0:\n",
    "            # pad to ctx_len\n",
    "            extended_tokenized_text = torch.cat([tokenized_text, torch.zeros(ctx_len-1, dtype=torch.long)])\n",
    "            # for each sequence, the first ctx_len tokens are the input, the last token is the output\n",
    "            X.extend(extended_tokenized_text[:-1].unfold(0, ctx_len, 1))\n",
    "            Y.extend(extended_tokenized_text[1:].unfold(0, ctx_len, 1))\n",
    "    # X_tensor in (n_sequences, ctx_len)\n",
    "    X_tensor = torch.stack(X)\n",
    "    # Y_tensor in (n_sequences, ctx_len)\n",
    "    Y_tensor = torch.stack(Y)\n",
    "    return X_tensor, Y_tensor\n",
    "\n",
    "\n",
    "def train(model:Transformer, optimizer:torch.optim.Optimizer, dataloader:DataLoader):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "    for X, Y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        Y_pred = model(X)\n",
    "        # we only compute loss where pad is not the target\n",
    "        loss = F.cross_entropy(Y_pred.reshape(-1, model.config.vocab_size), Y.reshape(-1), reduction='none')\n",
    "        loss = loss[Y.reshape(-1) != 0].mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_config = TransformerConfig(\n",
    "    d_embed=64,\n",
    "    d_k=16,\n",
    "    ctx_len=128,\n",
    "    n_heads=2,\n",
    "    ff_width=256,\n",
    "    n_blocks=3,\n",
    "    vocab_size=len(vocab)\n",
    ")\n",
    "\n",
    "model = Transformer(model_config).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.85148286819458\n",
      "loss: 4.8380537033081055\n",
      "loss: 4.823774814605713\n",
      "loss: 4.806513786315918\n",
      "loss: 4.792239665985107\n",
      "loss: 4.780441761016846\n",
      "loss: 4.766773223876953\n",
      "loss: 4.750765800476074\n",
      "loss: 4.737964153289795\n",
      "loss: 4.726701259613037\n",
      "loss: 4.714028358459473\n",
      "loss: 4.698774814605713\n",
      "loss: 4.683662414550781\n",
      "loss: 4.670864105224609\n",
      "loss: 4.655983924865723\n",
      "loss: 4.64409065246582\n",
      "loss: 4.631959438323975\n",
      "loss: 4.6183576583862305\n",
      "loss: 4.60421895980835\n",
      "loss: 4.5921430587768555\n",
      "loss: 4.578943252563477\n",
      "loss: 4.56453800201416\n",
      "loss: 4.5494537353515625\n",
      "loss: 4.540858745574951\n",
      "loss: 4.525060176849365\n",
      "loss: 4.513139247894287\n",
      "loss: 4.502863883972168\n",
      "loss: 4.490464210510254\n",
      "loss: 4.4762396812438965\n",
      "loss: 4.467777252197266\n",
      "loss: 4.455037593841553\n",
      "loss: 4.439979076385498\n",
      "loss: 4.424402236938477\n",
      "loss: 4.418720245361328\n",
      "loss: 4.401554107666016\n",
      "loss: 4.389864921569824\n",
      "loss: 4.378308296203613\n",
      "loss: 4.362484931945801\n",
      "loss: 4.351709365844727\n",
      "loss: 4.336392879486084\n",
      "loss: 4.321225166320801\n",
      "loss: 4.312399864196777\n",
      "loss: 4.30264949798584\n",
      "loss: 4.2885355949401855\n",
      "loss: 4.2728729248046875\n",
      "loss: 4.260067939758301\n",
      "loss: 4.247617721557617\n",
      "loss: 4.234596252441406\n",
      "loss: 4.2239155769348145\n",
      "loss: 4.215579986572266\n",
      "loss: 4.193680286407471\n",
      "loss: 4.181997299194336\n",
      "loss: 4.173483848571777\n",
      "loss: 4.157382011413574\n",
      "loss: 4.141425609588623\n",
      "loss: 4.133848190307617\n",
      "loss: 4.118869781494141\n",
      "loss: 4.105056285858154\n",
      "loss: 4.095104217529297\n",
      "loss: 4.078673839569092\n",
      "loss: 4.066617965698242\n",
      "loss: 4.054187774658203\n",
      "loss: 4.042409896850586\n",
      "loss: 4.029494285583496\n",
      "loss: 4.017851829528809\n",
      "loss: 4.002960681915283\n",
      "loss: 3.987506151199341\n",
      "loss: 3.9811460971832275\n",
      "loss: 3.9683690071105957\n",
      "loss: 3.963073253631592\n",
      "loss: 3.9463396072387695\n",
      "loss: 3.932913303375244\n",
      "loss: 3.9122486114501953\n",
      "loss: 3.9040520191192627\n",
      "loss: 3.89530086517334\n",
      "loss: 3.878701686859131\n",
      "loss: 3.8716483116149902\n",
      "loss: 3.8547439575195312\n",
      "loss: 3.8436944484710693\n",
      "loss: 3.833596706390381\n",
      "loss: 3.817786693572998\n",
      "loss: 3.806154727935791\n",
      "loss: 3.8019144535064697\n",
      "loss: 3.7888059616088867\n",
      "loss: 3.7773022651672363\n",
      "loss: 3.763798475265503\n",
      "loss: 3.75716495513916\n",
      "loss: 3.7342286109924316\n",
      "loss: 3.730803966522217\n",
      "loss: 3.7164711952209473\n",
      "loss: 3.712019920349121\n",
      "loss: 3.6954238414764404\n",
      "loss: 3.685678482055664\n",
      "loss: 3.681121826171875\n",
      "loss: 3.675546884536743\n",
      "loss: 3.653949022293091\n",
      "loss: 3.6538217067718506\n",
      "loss: 3.6427059173583984\n",
      "loss: 3.6334235668182373\n",
      "loss: 3.6231801509857178\n",
      "loss: 3.6046760082244873\n",
      "loss: 3.5960679054260254\n",
      "loss: 3.5857958793640137\n",
      "loss: 3.587472438812256\n",
      "loss: 3.565354585647583\n",
      "loss: 3.559075355529785\n",
      "loss: 3.5513594150543213\n",
      "loss: 3.547236919403076\n",
      "loss: 3.5268263816833496\n",
      "loss: 3.5283021926879883\n",
      "loss: 3.5136594772338867\n",
      "loss: 3.519791603088379\n",
      "loss: 3.498291492462158\n",
      "loss: 3.5024962425231934\n",
      "loss: 3.485351800918579\n",
      "loss: 3.481900215148926\n",
      "loss: 3.4872212409973145\n",
      "loss: 3.4727749824523926\n",
      "loss: 3.4566774368286133\n",
      "loss: 3.451408863067627\n",
      "loss: 3.4410765171051025\n",
      "loss: 3.434577703475952\n",
      "loss: 3.4298176765441895\n",
      "loss: 3.421015739440918\n",
      "loss: 3.4127469062805176\n",
      "loss: 3.4117116928100586\n",
      "loss: 3.397300958633423\n",
      "loss: 3.397733449935913\n",
      "loss: 3.3829023838043213\n",
      "loss: 3.3911752700805664\n",
      "loss: 3.380957841873169\n",
      "loss: 3.3742406368255615\n",
      "loss: 3.3714683055877686\n",
      "loss: 3.3548061847686768\n",
      "loss: 3.3547258377075195\n",
      "loss: 3.346191167831421\n",
      "loss: 3.3532052040100098\n",
      "loss: 3.343860626220703\n",
      "loss: 3.33762788772583\n",
      "loss: 3.325530529022217\n",
      "loss: 3.319941759109497\n",
      "loss: 3.306739330291748\n",
      "loss: 3.321877956390381\n",
      "loss: 3.308882713317871\n",
      "loss: 3.300983428955078\n",
      "loss: 3.3067073822021484\n",
      "loss: 3.293123245239258\n",
      "loss: 3.2921934127807617\n",
      "loss: 3.285290241241455\n",
      "loss: 3.281511068344116\n",
      "loss: 3.2682933807373047\n",
      "loss: 3.2662248611450195\n",
      "loss: 3.2698514461517334\n",
      "loss: 3.2530641555786133\n",
      "loss: 3.2644588947296143\n",
      "loss: 3.2328481674194336\n",
      "loss: 3.242356538772583\n",
      "loss: 3.2353453636169434\n",
      "loss: 3.2485604286193848\n",
      "loss: 3.2359962463378906\n",
      "loss: 3.2350058555603027\n",
      "loss: 3.2138614654541016\n",
      "loss: 3.2222366333007812\n",
      "loss: 3.2169570922851562\n",
      "loss: 3.2136714458465576\n",
      "loss: 3.198915719985962\n",
      "loss: 3.202712059020996\n",
      "loss: 3.203488349914551\n",
      "loss: 3.2087514400482178\n",
      "loss: 3.200099468231201\n",
      "loss: 3.188154697418213\n",
      "loss: 3.1871204376220703\n",
      "loss: 3.175693988800049\n",
      "loss: 3.173764944076538\n",
      "loss: 3.17682147026062\n",
      "loss: 3.161968946456909\n",
      "loss: 3.1676673889160156\n",
      "loss: 3.1550745964050293\n",
      "loss: 3.154118061065674\n",
      "loss: 3.1558241844177246\n",
      "loss: 3.146695137023926\n",
      "loss: 3.1471352577209473\n",
      "loss: 3.138519525527954\n",
      "loss: 3.1484627723693848\n",
      "loss: 3.141716241836548\n",
      "loss: 3.1308562755584717\n",
      "loss: 3.123039484024048\n",
      "loss: 3.1263461112976074\n",
      "loss: 3.126368522644043\n",
      "loss: 3.120466947555542\n",
      "loss: 3.118582248687744\n",
      "loss: 3.1273374557495117\n",
      "loss: 3.1039116382598877\n",
      "loss: 3.1150670051574707\n",
      "loss: 3.106743097305298\n",
      "loss: 3.1031241416931152\n",
      "loss: 3.0970120429992676\n",
      "loss: 3.096090316772461\n",
      "loss: 3.0846776962280273\n",
      "loss: 3.0887868404388428\n",
      "loss: 3.090083122253418\n",
      "loss: 3.087083339691162\n",
      "loss: 3.0821328163146973\n",
      "loss: 3.074644088745117\n",
      "loss: 3.0705206394195557\n",
      "loss: 3.0703022480010986\n",
      "loss: 3.0596401691436768\n",
      "loss: 3.07283353805542\n",
      "loss: 3.0573320388793945\n",
      "loss: 3.0648725032806396\n",
      "loss: 3.0535311698913574\n",
      "loss: 3.039517402648926\n",
      "loss: 3.0567996501922607\n",
      "loss: 3.0365145206451416\n",
      "loss: 3.0443832874298096\n",
      "loss: 3.041557788848877\n",
      "loss: 3.0333845615386963\n",
      "loss: 3.0366029739379883\n",
      "loss: 3.026385545730591\n",
      "loss: 3.0375075340270996\n",
      "loss: 3.0298452377319336\n",
      "loss: 3.021847724914551\n",
      "loss: 3.021988868713379\n",
      "loss: 3.031541585922241\n",
      "loss: 3.0230488777160645\n",
      "loss: 3.011155128479004\n",
      "loss: 3.0144948959350586\n",
      "loss: 3.0138535499572754\n",
      "loss: 3.008972644805908\n",
      "loss: 3.000670909881592\n",
      "loss: 3.001053810119629\n",
      "loss: 3.0057647228240967\n",
      "loss: 2.9967963695526123\n",
      "loss: 3.0040833950042725\n",
      "loss: 2.998159408569336\n",
      "loss: 2.988075017929077\n",
      "loss: 2.982780694961548\n",
      "loss: 2.987253427505493\n",
      "loss: 2.984236478805542\n",
      "loss: 2.977530002593994\n",
      "loss: 2.981513261795044\n",
      "loss: 2.9878530502319336\n",
      "loss: 2.9795215129852295\n",
      "loss: 2.976641893386841\n",
      "loss: 2.9623687267303467\n",
      "loss: 2.9628872871398926\n",
      "loss: 2.950901508331299\n",
      "loss: 2.9625258445739746\n",
      "loss: 2.9664716720581055\n",
      "loss: 2.9477698802948\n",
      "loss: 2.956941604614258\n",
      "loss: 2.9477732181549072\n",
      "loss: 2.959092140197754\n",
      "loss: 2.9396493434906006\n",
      "loss: 2.942516803741455\n",
      "loss: 2.9470419883728027\n",
      "loss: 2.944364070892334\n",
      "loss: 2.945220470428467\n",
      "loss: 2.933718204498291\n",
      "loss: 2.930652141571045\n",
      "loss: 2.932180881500244\n",
      "loss: 2.9357590675354004\n",
      "loss: 2.9260692596435547\n",
      "loss: 2.9320030212402344\n",
      "loss: 2.9244656562805176\n",
      "loss: 2.918971061706543\n",
      "loss: 2.9294016361236572\n",
      "loss: 2.910358428955078\n",
      "loss: 2.9199717044830322\n",
      "loss: 2.917870283126831\n",
      "loss: 2.914494514465332\n",
      "loss: 2.913911819458008\n",
      "loss: 2.9090585708618164\n",
      "loss: 2.905355215072632\n",
      "loss: 2.9089818000793457\n",
      "loss: 2.905085802078247\n",
      "loss: 2.900479316711426\n",
      "loss: 2.8978779315948486\n",
      "loss: 2.9063007831573486\n",
      "loss: 2.8927316665649414\n",
      "loss: 2.8962008953094482\n",
      "loss: 2.900590419769287\n",
      "loss: 2.892470598220825\n",
      "loss: 2.881542682647705\n",
      "loss: 2.8914952278137207\n",
      "loss: 2.900038480758667\n",
      "loss: 2.892676591873169\n",
      "loss: 2.8826873302459717\n",
      "loss: 2.878161907196045\n",
      "loss: 2.878471612930298\n",
      "loss: 2.880554676055908\n",
      "loss: 2.8810908794403076\n",
      "loss: 2.8742222785949707\n",
      "loss: 2.872922420501709\n",
      "loss: 2.867863178253174\n",
      "loss: 2.8740923404693604\n",
      "loss: 2.8695013523101807\n",
      "loss: 2.8681344985961914\n",
      "loss: 2.8609795570373535\n",
      "loss: 2.8650712966918945\n",
      "loss: 2.8600552082061768\n",
      "loss: 2.8618040084838867\n",
      "loss: 2.8534610271453857\n",
      "loss: 2.855081081390381\n",
      "loss: 2.849100112915039\n",
      "loss: 2.865541696548462\n",
      "loss: 2.8545660972595215\n",
      "loss: 2.842397689819336\n",
      "loss: 2.8543508052825928\n",
      "loss: 2.844970464706421\n",
      "loss: 2.848383665084839\n",
      "loss: 2.844116687774658\n",
      "loss: 2.844479560852051\n",
      "loss: 2.8372507095336914\n",
      "loss: 2.8418216705322266\n",
      "loss: 2.846676826477051\n",
      "loss: 2.8468074798583984\n",
      "loss: 2.841928482055664\n",
      "loss: 2.8388824462890625\n",
      "loss: 2.8390109539031982\n",
      "loss: 2.833683967590332\n",
      "loss: 2.8350319862365723\n",
      "loss: 2.831488609313965\n",
      "loss: 2.825073719024658\n",
      "loss: 2.8334906101226807\n",
      "loss: 2.824995994567871\n",
      "loss: 2.828308343887329\n",
      "loss: 2.829301118850708\n",
      "loss: 2.824833393096924\n",
      "loss: 2.8125364780426025\n",
      "loss: 2.8099923133850098\n",
      "loss: 2.817474126815796\n",
      "loss: 2.8165059089660645\n",
      "loss: 2.8177216053009033\n",
      "loss: 2.8171262741088867\n",
      "loss: 2.8125452995300293\n",
      "loss: 2.8101015090942383\n",
      "loss: 2.8092775344848633\n",
      "loss: 2.807703733444214\n",
      "loss: 2.810533046722412\n",
      "loss: 2.798961639404297\n",
      "loss: 2.811511278152466\n",
      "loss: 2.8053722381591797\n",
      "loss: 2.8053698539733887\n",
      "loss: 2.806272029876709\n",
      "loss: 2.801429033279419\n",
      "loss: 2.8011932373046875\n",
      "loss: 2.8049395084381104\n",
      "loss: 2.7983920574188232\n",
      "loss: 2.796342611312866\n",
      "loss: 2.796099901199341\n",
      "loss: 2.8010287284851074\n",
      "loss: 2.79355525970459\n",
      "loss: 2.789743423461914\n",
      "loss: 2.7902326583862305\n",
      "loss: 2.7918646335601807\n",
      "loss: 2.784294366836548\n",
      "loss: 2.7854461669921875\n",
      "loss: 2.796675205230713\n",
      "loss: 2.7828710079193115\n",
      "loss: 2.7859814167022705\n",
      "loss: 2.7760133743286133\n",
      "loss: 2.7889060974121094\n",
      "loss: 2.7771244049072266\n",
      "loss: 2.7888598442077637\n",
      "loss: 2.7691543102264404\n",
      "loss: 2.776423931121826\n",
      "loss: 2.7874512672424316\n",
      "loss: 2.768537998199463\n",
      "loss: 2.770461320877075\n",
      "loss: 2.768922805786133\n",
      "loss: 2.769865036010742\n",
      "loss: 2.7769222259521484\n",
      "loss: 2.771073579788208\n",
      "loss: 2.766226291656494\n",
      "loss: 2.7672481536865234\n",
      "loss: 2.7682695388793945\n",
      "loss: 2.767512559890747\n",
      "loss: 2.7563531398773193\n",
      "loss: 2.7668089866638184\n",
      "loss: 2.7666187286376953\n",
      "loss: 2.7585248947143555\n",
      "loss: 2.7586352825164795\n",
      "loss: 2.7663984298706055\n",
      "loss: 2.7620186805725098\n",
      "loss: 2.766267776489258\n",
      "loss: 2.7562546730041504\n",
      "loss: 2.752150058746338\n",
      "loss: 2.7595174312591553\n",
      "loss: 2.756265640258789\n",
      "loss: 2.753269672393799\n",
      "loss: 2.7460031509399414\n",
      "loss: 2.7490739822387695\n",
      "loss: 2.752393960952759\n",
      "loss: 2.74981689453125\n",
      "loss: 2.7470040321350098\n",
      "loss: 2.7428512573242188\n",
      "loss: 2.745232105255127\n",
      "loss: 2.745117664337158\n",
      "loss: 2.7470242977142334\n",
      "loss: 2.7319774627685547\n",
      "loss: 2.739236831665039\n",
      "loss: 2.743622303009033\n",
      "loss: 2.7459819316864014\n",
      "loss: 2.736652374267578\n",
      "loss: 2.734210968017578\n",
      "loss: 2.735543727874756\n",
      "loss: 2.7288358211517334\n",
      "loss: 2.7330965995788574\n",
      "loss: 2.735959053039551\n",
      "loss: 2.732632637023926\n",
      "loss: 2.732391834259033\n",
      "loss: 2.727999687194824\n",
      "loss: 2.7332653999328613\n",
      "loss: 2.7291464805603027\n",
      "loss: 2.7294373512268066\n",
      "loss: 2.7264323234558105\n",
      "loss: 2.732466697692871\n",
      "loss: 2.7210922241210938\n",
      "loss: 2.727630138397217\n",
      "loss: 2.722686290740967\n",
      "loss: 2.71340274810791\n",
      "loss: 2.7247605323791504\n",
      "loss: 2.7244811058044434\n",
      "loss: 2.71417236328125\n",
      "loss: 2.7193264961242676\n",
      "loss: 2.7179465293884277\n",
      "loss: 2.712620258331299\n",
      "loss: 2.7184574604034424\n",
      "loss: 2.7152886390686035\n",
      "loss: 2.7165048122406006\n",
      "loss: 2.7086727619171143\n",
      "loss: 2.716907262802124\n",
      "loss: 2.719532012939453\n",
      "loss: 2.713705539703369\n",
      "loss: 2.712984800338745\n",
      "loss: 2.7077271938323975\n",
      "loss: 2.70808482170105\n",
      "loss: 2.7054333686828613\n",
      "loss: 2.7056984901428223\n",
      "loss: 2.71012020111084\n",
      "loss: 2.7057316303253174\n",
      "loss: 2.7041358947753906\n",
      "loss: 2.7036895751953125\n",
      "loss: 2.7090201377868652\n",
      "loss: 2.7029666900634766\n",
      "loss: 2.697258472442627\n",
      "loss: 2.697643280029297\n",
      "loss: 2.7075555324554443\n",
      "loss: 2.696406126022339\n",
      "loss: 2.7037062644958496\n",
      "loss: 2.6930668354034424\n",
      "loss: 2.69801664352417\n",
      "loss: 2.694025993347168\n",
      "loss: 2.688939094543457\n",
      "loss: 2.6928510665893555\n",
      "loss: 2.6827468872070312\n",
      "loss: 2.687549114227295\n",
      "loss: 2.689014434814453\n",
      "loss: 2.686779737472534\n",
      "loss: 2.6873528957366943\n",
      "loss: 2.6931076049804688\n",
      "loss: 2.682598352432251\n",
      "loss: 2.6820688247680664\n",
      "loss: 2.6721179485321045\n",
      "loss: 2.6868181228637695\n",
      "loss: 2.6914942264556885\n",
      "loss: 2.678152561187744\n",
      "loss: 2.677868366241455\n",
      "loss: 2.6856436729431152\n",
      "loss: 2.672694206237793\n",
      "loss: 2.6851882934570312\n",
      "loss: 2.684284210205078\n",
      "loss: 2.682215929031372\n",
      "loss: 2.6727609634399414\n",
      "loss: 2.6795105934143066\n",
      "loss: 2.6757664680480957\n",
      "loss: 2.676107406616211\n",
      "loss: 2.6733877658843994\n",
      "loss: 2.6765847206115723\n",
      "loss: 2.676720142364502\n",
      "loss: 2.6709625720977783\n",
      "loss: 2.666322708129883\n",
      "loss: 2.6719179153442383\n",
      "loss: 2.6698577404022217\n",
      "loss: 2.6712498664855957\n",
      "loss: 2.670367956161499\n",
      "loss: 2.6641905307769775\n",
      "loss: 2.6727399826049805\n",
      "loss: 2.6680002212524414\n",
      "loss: 2.6740798950195312\n",
      "loss: 2.6664938926696777\n",
      "loss: 2.6676650047302246\n",
      "loss: 2.671659469604492\n",
      "loss: 2.662745952606201\n",
      "loss: 2.663839340209961\n",
      "loss: 2.662013053894043\n",
      "loss: 2.663055896759033\n",
      "loss: 2.6588125228881836\n",
      "loss: 2.661207675933838\n",
      "loss: 2.6557352542877197\n",
      "loss: 2.6579885482788086\n",
      "loss: 2.6678578853607178\n",
      "loss: 2.657735824584961\n",
      "loss: 2.6628530025482178\n",
      "loss: 2.6564695835113525\n",
      "loss: 2.6508941650390625\n",
      "loss: 2.6532280445098877\n",
      "loss: 2.652831554412842\n",
      "loss: 2.654501438140869\n",
      "loss: 2.6524922847747803\n",
      "loss: 2.6544954776763916\n",
      "loss: 2.6577610969543457\n",
      "loss: 2.65031099319458\n",
      "loss: 2.6435065269470215\n",
      "loss: 2.6500070095062256\n",
      "loss: 2.651637077331543\n",
      "loss: 2.646115779876709\n",
      "loss: 2.6443705558776855\n",
      "loss: 2.6436338424682617\n",
      "loss: 2.637876510620117\n",
      "loss: 2.6405653953552246\n",
      "loss: 2.647484302520752\n",
      "loss: 2.6409268379211426\n",
      "loss: 2.643059253692627\n",
      "loss: 2.6441287994384766\n",
      "loss: 2.6470391750335693\n",
      "loss: 2.652195453643799\n",
      "loss: 2.6362009048461914\n",
      "loss: 2.637814521789551\n",
      "loss: 2.636991024017334\n",
      "loss: 2.6426615715026855\n",
      "loss: 2.6454343795776367\n",
      "loss: 2.632467269897461\n",
      "loss: 2.638697624206543\n",
      "loss: 2.6395044326782227\n",
      "loss: 2.6381995677948\n",
      "loss: 2.6393847465515137\n",
      "loss: 2.6365175247192383\n",
      "loss: 2.639227867126465\n",
      "loss: 2.63577938079834\n",
      "loss: 2.6301417350769043\n",
      "loss: 2.6379010677337646\n",
      "loss: 2.623056411743164\n",
      "loss: 2.627512216567993\n",
      "loss: 2.6389541625976562\n",
      "loss: 2.6310982704162598\n",
      "loss: 2.6240687370300293\n",
      "loss: 2.634176731109619\n",
      "loss: 2.6280064582824707\n",
      "loss: 2.6267666816711426\n",
      "loss: 2.6300554275512695\n",
      "loss: 2.6262664794921875\n",
      "loss: 2.6234288215637207\n",
      "loss: 2.62252140045166\n",
      "loss: 2.6304264068603516\n",
      "loss: 2.6255581378936768\n",
      "loss: 2.6221485137939453\n",
      "loss: 2.6359429359436035\n",
      "loss: 2.617563247680664\n",
      "loss: 2.626647472381592\n",
      "loss: 2.618516445159912\n",
      "loss: 2.618474006652832\n",
      "loss: 2.6225080490112305\n",
      "loss: 2.625093936920166\n",
      "loss: 2.6293156147003174\n",
      "loss: 2.6125874519348145\n",
      "loss: 2.6153547763824463\n",
      "loss: 2.618424892425537\n",
      "loss: 2.6234500408172607\n",
      "loss: 2.614989757537842\n",
      "loss: 2.617779016494751\n",
      "loss: 2.6141724586486816\n",
      "loss: 2.6151161193847656\n",
      "loss: 2.620882034301758\n",
      "loss: 2.61940336227417\n",
      "loss: 2.611987590789795\n",
      "loss: 2.618381977081299\n",
      "loss: 2.6260697841644287\n",
      "loss: 2.6161155700683594\n",
      "loss: 2.6104419231414795\n",
      "loss: 2.6173887252807617\n",
      "loss: 2.6028006076812744\n",
      "loss: 2.618701457977295\n",
      "loss: 2.6037158966064453\n",
      "loss: 2.6122663021087646\n",
      "loss: 2.6109278202056885\n",
      "loss: 2.6128218173980713\n",
      "loss: 2.605487823486328\n",
      "loss: 2.607102394104004\n",
      "loss: 2.602722644805908\n",
      "loss: 2.608849048614502\n",
      "loss: 2.607485055923462\n",
      "loss: 2.6067585945129395\n",
      "loss: 2.6089024543762207\n",
      "loss: 2.6035141944885254\n",
      "loss: 2.596682548522949\n",
      "loss: 2.5923776626586914\n",
      "loss: 2.607163906097412\n",
      "loss: 2.6002655029296875\n",
      "loss: 2.6035215854644775\n",
      "loss: 2.602125644683838\n",
      "loss: 2.603820562362671\n",
      "loss: 2.5920662879943848\n",
      "loss: 2.5959863662719727\n",
      "loss: 2.6028847694396973\n",
      "loss: 2.5954456329345703\n",
      "loss: 2.600132942199707\n",
      "loss: 2.5979719161987305\n",
      "loss: 2.605771541595459\n",
      "loss: 2.5947189331054688\n",
      "loss: 2.599132776260376\n",
      "loss: 2.5987210273742676\n",
      "loss: 2.601378917694092\n",
      "loss: 2.598952293395996\n",
      "loss: 2.601634979248047\n",
      "loss: 2.5983924865722656\n",
      "loss: 2.595580577850342\n",
      "loss: 2.5926036834716797\n",
      "loss: 2.6004724502563477\n",
      "loss: 2.599590301513672\n",
      "loss: 2.597381114959717\n",
      "loss: 2.5980875492095947\n",
      "loss: 2.5906176567077637\n",
      "loss: 2.5978565216064453\n",
      "loss: 2.584102153778076\n",
      "loss: 2.591691255569458\n",
      "loss: 2.590221643447876\n",
      "loss: 2.5877480506896973\n",
      "loss: 2.5922741889953613\n",
      "loss: 2.5928354263305664\n",
      "loss: 2.590097427368164\n",
      "loss: 2.5881247520446777\n",
      "loss: 2.5893187522888184\n",
      "loss: 2.589454412460327\n",
      "loss: 2.601118326187134\n",
      "loss: 2.5914828777313232\n",
      "loss: 2.589925527572632\n",
      "loss: 2.586315631866455\n",
      "loss: 2.592461347579956\n",
      "loss: 2.5777320861816406\n",
      "loss: 2.585675001144409\n",
      "loss: 2.583218574523926\n",
      "loss: 2.5864851474761963\n",
      "loss: 2.591856002807617\n",
      "loss: 2.5924618244171143\n",
      "loss: 2.5837583541870117\n",
      "loss: 2.586909770965576\n",
      "loss: 2.579864978790283\n",
      "loss: 2.5809404850006104\n",
      "loss: 2.5834178924560547\n",
      "loss: 2.589219093322754\n",
      "loss: 2.5839531421661377\n",
      "loss: 2.5755224227905273\n",
      "loss: 2.5911951065063477\n",
      "loss: 2.586697578430176\n",
      "loss: 2.582054615020752\n",
      "loss: 2.577533006668091\n",
      "loss: 2.5841283798217773\n",
      "loss: 2.5778350830078125\n",
      "loss: 2.5777859687805176\n",
      "loss: 2.58353590965271\n",
      "loss: 2.5780720710754395\n",
      "loss: 2.57918119430542\n",
      "loss: 2.588916540145874\n",
      "loss: 2.578009605407715\n",
      "loss: 2.5740277767181396\n",
      "loss: 2.580091953277588\n",
      "loss: 2.5722155570983887\n",
      "loss: 2.5787100791931152\n",
      "loss: 2.5746731758117676\n",
      "loss: 2.576916217803955\n",
      "loss: 2.5752859115600586\n",
      "loss: 2.5807695388793945\n",
      "loss: 2.572922468185425\n",
      "loss: 2.579524517059326\n",
      "loss: 2.5683939456939697\n",
      "loss: 2.581838607788086\n",
      "loss: 2.570876359939575\n",
      "loss: 2.575751304626465\n",
      "loss: 2.575428009033203\n",
      "loss: 2.573345184326172\n",
      "loss: 2.5769593715667725\n",
      "loss: 2.5650997161865234\n",
      "loss: 2.568986415863037\n",
      "loss: 2.5683350563049316\n",
      "loss: 2.5764834880828857\n",
      "loss: 2.570385456085205\n",
      "loss: 2.572178363800049\n",
      "loss: 2.5686440467834473\n",
      "loss: 2.5588788986206055\n",
      "loss: 2.5678248405456543\n",
      "loss: 2.5722856521606445\n",
      "loss: 2.5693283081054688\n",
      "loss: 2.5650413036346436\n",
      "loss: 2.5743470191955566\n",
      "loss: 2.569054126739502\n",
      "loss: 2.565157175064087\n",
      "loss: 2.564516544342041\n",
      "loss: 2.564746379852295\n",
      "loss: 2.5681018829345703\n",
      "loss: 2.5693445205688477\n",
      "loss: 2.5668437480926514\n",
      "loss: 2.5637388229370117\n",
      "loss: 2.5659446716308594\n",
      "loss: 2.560783863067627\n",
      "loss: 2.565523386001587\n",
      "loss: 2.5669589042663574\n",
      "loss: 2.5586085319519043\n",
      "loss: 2.556976318359375\n",
      "loss: 2.5587158203125\n",
      "loss: 2.5669307708740234\n",
      "loss: 2.564054489135742\n",
      "loss: 2.5638978481292725\n",
      "loss: 2.5669586658477783\n",
      "loss: 2.566256523132324\n",
      "loss: 2.563796043395996\n",
      "loss: 2.5635621547698975\n",
      "loss: 2.562063694000244\n",
      "loss: 2.5661230087280273\n",
      "loss: 2.560009479522705\n",
      "loss: 2.561035633087158\n",
      "loss: 2.562497138977051\n",
      "loss: 2.554384469985962\n",
      "loss: 2.559703826904297\n",
      "loss: 2.5548036098480225\n",
      "loss: 2.5572774410247803\n",
      "loss: 2.5619685649871826\n",
      "loss: 2.5580203533172607\n",
      "loss: 2.5667624473571777\n",
      "loss: 2.5553417205810547\n",
      "loss: 2.563371419906616\n",
      "loss: 2.555911064147949\n",
      "loss: 2.558307647705078\n",
      "loss: 2.562966823577881\n",
      "loss: 2.553476095199585\n",
      "loss: 2.556480884552002\n",
      "loss: 2.553853988647461\n",
      "loss: 2.552678108215332\n",
      "loss: 2.5541908740997314\n",
      "loss: 2.5507009029388428\n",
      "loss: 2.5464987754821777\n",
      "loss: 2.557305335998535\n",
      "loss: 2.5578360557556152\n",
      "loss: 2.546311140060425\n",
      "loss: 2.547804355621338\n",
      "loss: 2.5556061267852783\n",
      "loss: 2.5526504516601562\n",
      "loss: 2.5508131980895996\n",
      "loss: 2.543623447418213\n",
      "loss: 2.5443367958068848\n",
      "loss: 2.5496788024902344\n",
      "loss: 2.558912754058838\n",
      "loss: 2.554830312728882\n",
      "loss: 2.547969341278076\n",
      "loss: 2.5466723442077637\n",
      "loss: 2.5529723167419434\n",
      "loss: 2.5496020317077637\n",
      "loss: 2.5476417541503906\n",
      "loss: 2.5565004348754883\n",
      "loss: 2.5567424297332764\n",
      "loss: 2.5457191467285156\n",
      "loss: 2.554363250732422\n",
      "loss: 2.5513269901275635\n",
      "loss: 2.552367687225342\n",
      "loss: 2.5517144203186035\n",
      "loss: 2.5426478385925293\n",
      "loss: 2.543961763381958\n",
      "loss: 2.5511014461517334\n",
      "loss: 2.54337739944458\n",
      "loss: 2.552375078201294\n",
      "loss: 2.5426084995269775\n",
      "loss: 2.5517566204071045\n",
      "loss: 2.5441997051239014\n",
      "loss: 2.549643039703369\n",
      "loss: 2.5427234172821045\n",
      "loss: 2.5435309410095215\n",
      "loss: 2.5529160499572754\n",
      "loss: 2.5492959022521973\n",
      "loss: 2.548232078552246\n",
      "loss: 2.54947566986084\n",
      "loss: 2.546783447265625\n",
      "loss: 2.545442581176758\n",
      "loss: 2.5449931621551514\n",
      "loss: 2.5409135818481445\n",
      "loss: 2.5443389415740967\n",
      "loss: 2.546682834625244\n",
      "loss: 2.5425288677215576\n",
      "loss: 2.535552978515625\n",
      "loss: 2.539001941680908\n",
      "loss: 2.5365171432495117\n",
      "loss: 2.5429916381835938\n",
      "loss: 2.543692111968994\n",
      "loss: 2.5489823818206787\n",
      "loss: 2.5370848178863525\n",
      "loss: 2.5395398139953613\n",
      "loss: 2.5417490005493164\n",
      "loss: 2.531844139099121\n",
      "loss: 2.5419416427612305\n",
      "loss: 2.5434744358062744\n",
      "loss: 2.538483142852783\n",
      "loss: 2.5347938537597656\n",
      "loss: 2.5421907901763916\n",
      "loss: 2.5371313095092773\n",
      "loss: 2.5450942516326904\n",
      "loss: 2.535773754119873\n",
      "loss: 2.5327181816101074\n",
      "loss: 2.540783643722534\n",
      "loss: 2.540660858154297\n",
      "loss: 2.537522792816162\n",
      "loss: 2.5364818572998047\n",
      "loss: 2.5352954864501953\n",
      "loss: 2.5452351570129395\n",
      "loss: 2.5404672622680664\n",
      "loss: 2.541835308074951\n",
      "loss: 2.539712905883789\n",
      "loss: 2.537468194961548\n",
      "loss: 2.537168502807617\n",
      "loss: 2.53389835357666\n",
      "loss: 2.5410571098327637\n",
      "loss: 2.535609722137451\n",
      "loss: 2.535142660140991\n",
      "loss: 2.5328807830810547\n",
      "loss: 2.5313382148742676\n",
      "loss: 2.5377559661865234\n",
      "loss: 2.534993886947632\n",
      "loss: 2.5383365154266357\n",
      "loss: 2.540203094482422\n",
      "loss: 2.5355634689331055\n",
      "loss: 2.538203239440918\n",
      "loss: 2.5330252647399902\n",
      "loss: 2.529268741607666\n",
      "loss: 2.5317134857177734\n",
      "loss: 2.5396604537963867\n",
      "loss: 2.540355682373047\n",
      "loss: 2.5342071056365967\n",
      "loss: 2.528618812561035\n",
      "loss: 2.5336759090423584\n",
      "loss: 2.5301666259765625\n",
      "loss: 2.5331027507781982\n",
      "loss: 2.541050434112549\n",
      "loss: 2.5326743125915527\n",
      "loss: 2.532883644104004\n",
      "loss: 2.5285074710845947\n",
      "loss: 2.533447265625\n",
      "loss: 2.529111623764038\n",
      "loss: 2.5295934677124023\n",
      "loss: 2.5282740592956543\n",
      "loss: 2.527153968811035\n",
      "loss: 2.5300326347351074\n",
      "loss: 2.538395643234253\n",
      "loss: 2.5326812267303467\n",
      "loss: 2.5321500301361084\n",
      "loss: 2.525355815887451\n",
      "loss: 2.5234313011169434\n",
      "loss: 2.534529685974121\n",
      "loss: 2.5334670543670654\n",
      "loss: 2.5379538536071777\n",
      "loss: 2.5309572219848633\n",
      "loss: 2.5302908420562744\n",
      "loss: 2.523627758026123\n",
      "loss: 2.5274572372436523\n",
      "loss: 2.531527042388916\n",
      "loss: 2.5268688201904297\n",
      "loss: 2.529604911804199\n",
      "loss: 2.5293116569519043\n",
      "loss: 2.52793025970459\n",
      "loss: 2.533454179763794\n",
      "loss: 2.5314064025878906\n",
      "loss: 2.5230565071105957\n",
      "loss: 2.5191893577575684\n",
      "loss: 2.5238113403320312\n",
      "loss: 2.5211730003356934\n",
      "loss: 2.5307350158691406\n",
      "loss: 2.5222208499908447\n",
      "loss: 2.527904510498047\n",
      "loss: 2.5259552001953125\n",
      "loss: 2.526197910308838\n",
      "loss: 2.5254266262054443\n",
      "loss: 2.5190114974975586\n",
      "loss: 2.5200815200805664\n",
      "loss: 2.5212385654449463\n",
      "loss: 2.5194551944732666\n",
      "loss: 2.525240421295166\n",
      "loss: 2.5226430892944336\n",
      "loss: 2.5207138061523438\n",
      "loss: 2.5249485969543457\n",
      "loss: 2.5270535945892334\n",
      "loss: 2.518951177597046\n",
      "loss: 2.519148826599121\n",
      "loss: 2.522721290588379\n",
      "loss: 2.516496419906616\n",
      "loss: 2.5276427268981934\n",
      "loss: 2.513828754425049\n",
      "loss: 2.51692795753479\n",
      "loss: 2.5277352333068848\n",
      "loss: 2.5196244716644287\n",
      "loss: 2.5189695358276367\n",
      "loss: 2.521900177001953\n",
      "loss: 2.513928174972534\n",
      "loss: 2.522040843963623\n",
      "loss: 2.5236012935638428\n",
      "loss: 2.5191073417663574\n",
      "loss: 2.5195565223693848\n",
      "loss: 2.5224108695983887\n",
      "loss: 2.518843173980713\n",
      "loss: 2.5229790210723877\n",
      "loss: 2.5191187858581543\n",
      "loss: 2.5176777839660645\n",
      "loss: 2.5167551040649414\n",
      "loss: 2.5148098468780518\n",
      "loss: 2.5195631980895996\n",
      "loss: 2.5155749320983887\n",
      "loss: 2.513134479522705\n",
      "loss: 2.5141801834106445\n",
      "loss: 2.5197958946228027\n",
      "loss: 2.5133299827575684\n",
      "loss: 2.5187838077545166\n",
      "loss: 2.518357753753662\n",
      "loss: 2.5235912799835205\n",
      "loss: 2.516294240951538\n",
      "loss: 2.5134596824645996\n",
      "loss: 2.514009714126587\n",
      "loss: 2.5202765464782715\n",
      "loss: 2.5104763507843018\n",
      "loss: 2.516619920730591\n",
      "loss: 2.5153698921203613\n",
      "loss: 2.5180578231811523\n",
      "loss: 2.511317253112793\n",
      "loss: 2.514202833175659\n",
      "loss: 2.511559247970581\n",
      "loss: 2.5132369995117188\n",
      "loss: 2.5106887817382812\n",
      "loss: 2.5261199474334717\n",
      "loss: 2.51958966255188\n",
      "loss: 2.507981777191162\n",
      "loss: 2.509488582611084\n",
      "loss: 2.5121865272521973\n",
      "loss: 2.5101563930511475\n",
      "loss: 2.5179755687713623\n",
      "loss: 2.5123276710510254\n",
      "loss: 2.517103910446167\n",
      "loss: 2.5137176513671875\n",
      "loss: 2.5144546031951904\n",
      "loss: 2.504685878753662\n",
      "loss: 2.5155768394470215\n",
      "loss: 2.5181522369384766\n",
      "loss: 2.5092625617980957\n",
      "loss: 2.5083603858947754\n",
      "loss: 2.5116419792175293\n",
      "loss: 2.5116961002349854\n",
      "loss: 2.5080621242523193\n",
      "loss: 2.515145778656006\n",
      "loss: 2.519855499267578\n",
      "loss: 2.5130386352539062\n",
      "loss: 2.508693218231201\n",
      "loss: 2.5124425888061523\n",
      "loss: 2.5088398456573486\n",
      "loss: 2.5093865394592285\n",
      "loss: 2.5112147331237793\n",
      "loss: 2.509063959121704\n",
      "loss: 2.513275623321533\n",
      "loss: 2.511575937271118\n",
      "loss: 2.5122883319854736\n",
      "loss: 2.508857488632202\n",
      "loss: 2.5104780197143555\n",
      "loss: 2.4978394508361816\n",
      "loss: 2.511657238006592\n",
      "loss: 2.512794017791748\n",
      "loss: 2.512273073196411\n",
      "loss: 2.506427764892578\n",
      "loss: 2.5051116943359375\n",
      "loss: 2.5037057399749756\n",
      "loss: 2.5150656700134277\n",
      "loss: 2.51129412651062\n",
      "loss: 2.509882926940918\n",
      "loss: 2.505066394805908\n",
      "loss: 2.5064878463745117\n",
      "loss: 2.5011520385742188\n",
      "loss: 2.505981206893921\n",
      "loss: 2.511867046356201\n",
      "loss: 2.5157666206359863\n",
      "loss: 2.5080065727233887\n",
      "loss: 2.501143455505371\n",
      "loss: 2.5034170150756836\n",
      "loss: 2.510611057281494\n",
      "loss: 2.507716178894043\n",
      "loss: 2.5051424503326416\n",
      "loss: 2.5063936710357666\n",
      "loss: 2.509995937347412\n",
      "loss: 2.5064282417297363\n",
      "loss: 2.501779317855835\n",
      "loss: 2.5083394050598145\n",
      "loss: 2.505354166030884\n",
      "loss: 2.515859603881836\n",
      "loss: 2.5001728534698486\n",
      "loss: 2.5084102153778076\n",
      "loss: 2.501720905303955\n",
      "loss: 2.507920265197754\n",
      "loss: 2.507455348968506\n",
      "loss: 2.4980599880218506\n",
      "loss: 2.502213478088379\n",
      "loss: 2.5021166801452637\n",
      "loss: 2.5003280639648438\n",
      "loss: 2.5039005279541016\n",
      "loss: 2.5021963119506836\n",
      "loss: 2.5071094036102295\n",
      "loss: 2.5021517276763916\n",
      "loss: 2.5068271160125732\n",
      "loss: 2.505423069000244\n",
      "loss: 2.502642869949341\n",
      "loss: 2.500314235687256\n",
      "loss: 2.503798484802246\n",
      "loss: 2.4964730739593506\n",
      "loss: 2.504349708557129\n",
      "loss: 2.502906322479248\n",
      "loss: 2.5042078495025635\n",
      "loss: 2.4998087882995605\n",
      "loss: 2.498211622238159\n",
      "loss: 2.500514507293701\n",
      "loss: 2.504978895187378\n",
      "loss: 2.498321533203125\n",
      "loss: 2.501987934112549\n",
      "loss: 2.4950413703918457\n",
      "loss: 2.4959287643432617\n",
      "loss: 2.506621837615967\n",
      "loss: 2.494363307952881\n",
      "loss: 2.4963598251342773\n",
      "loss: 2.5021486282348633\n",
      "loss: 2.497927665710449\n",
      "loss: 2.5006637573242188\n",
      "loss: 2.502253293991089\n",
      "loss: 2.498325824737549\n",
      "loss: 2.4993553161621094\n",
      "loss: 2.503897190093994\n",
      "loss: 2.4937496185302734\n",
      "loss: 2.504981756210327\n",
      "loss: 2.497446060180664\n",
      "loss: 2.501702308654785\n",
      "loss: 2.504678726196289\n",
      "loss: 2.502972364425659\n",
      "loss: 2.4924488067626953\n",
      "loss: 2.5008487701416016\n",
      "loss: 2.4973702430725098\n",
      "loss: 2.5006473064422607\n",
      "loss: 2.4964206218719482\n",
      "loss: 2.4988620281219482\n",
      "loss: 2.49623441696167\n",
      "loss: 2.4967644214630127\n",
      "loss: 2.4994425773620605\n",
      "loss: 2.497077226638794\n",
      "loss: 2.4992258548736572\n",
      "loss: 2.502445697784424\n",
      "loss: 2.4961185455322266\n",
      "loss: 2.491931915283203\n",
      "loss: 2.4942946434020996\n",
      "loss: 2.4981751441955566\n",
      "loss: 2.495522975921631\n",
      "loss: 2.4949660301208496\n",
      "loss: 2.4955430030822754\n",
      "loss: 2.498450517654419\n",
      "loss: 2.49550724029541\n",
      "loss: 2.490779161453247\n",
      "loss: 2.492398738861084\n",
      "loss: 2.496981143951416\n",
      "loss: 2.502821683883667\n",
      "loss: 2.4936888217926025\n",
      "loss: 2.4926865100860596\n",
      "loss: 2.4948716163635254\n",
      "loss: 2.4918601512908936\n",
      "loss: 2.486403465270996\n",
      "loss: 2.494856834411621\n",
      "loss: 2.4893248081207275\n",
      "loss: 2.4963345527648926\n",
      "loss: 2.4961557388305664\n",
      "loss: 2.4936723709106445\n",
      "loss: 2.496494770050049\n",
      "loss: 2.486717700958252\n",
      "loss: 2.4971442222595215\n",
      "loss: 2.4930896759033203\n",
      "loss: 2.494935989379883\n",
      "loss: 2.485488176345825\n",
      "loss: 2.4978370666503906\n",
      "loss: 2.4916090965270996\n",
      "loss: 2.492187976837158\n",
      "loss: 2.4927358627319336\n",
      "loss: 2.493964195251465\n",
      "loss: 2.4944684505462646\n",
      "loss: 2.484212636947632\n",
      "loss: 2.4940414428710938\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "# read text sources from disk\n",
    "text_sources = []\n",
    "for filename in os.listdir(\"data\"):\n",
    "    with open(os.path.join(\"data\", filename), \"r\") as f:\n",
    "        text_sources.append(f.read())\n",
    "\n",
    "dataset = TensorDataset(*prep_training_data(text_sources, model_config.ctx_len))\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "train(model, optimizer, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def generate(model: Transformer, seed_str:str, length:int):\n",
    "    output = seed_str\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        # seed in (ctx_len)\n",
    "        seed = tokenize(seed_str).to(device)\n",
    "        seed = F.pad(seed, (model.config.ctx_len - len(seed), 0))\n",
    "        for _ in range(length):\n",
    "            # Y_pred in (ctx_len, vocab_size)\n",
    "            Y_pred = model(seed.unsqueeze(0))[0]\n",
    "            # pred_token in (1)\n",
    "            pred_token = torch.multinomial(F.softmax(Y_pred[-1], dim=-1), num_samples=1)\n",
    "            # seed in (ctx_len)\n",
    "            seed = torch.cat([seed[1:], pred_token], dim=-1)\n",
    "\n",
    "            output += inv_vocab[int(pred_token.item())]\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dogrn when h nd wiris hivend y ! pe imure Animpis t trufile, lemde t meeiveonove s, hen y ma bsth;\n",
      "F\n",
      "Th adoo'ldeis t thantirifot tetu ot'swerdoy J sapatraifou sircesty out a le,\n",
      "ARn m mousher d,\n",
      "\n",
      "\n",
      "Wis inthelang m weanops.\n",
      "\n",
      "Thiseg cow'd pllelingrin s caMat p d ausendithalshay LI t nty Yy go ay spf; ndung, in byo pld, w po ous br mid ovisud;\n",
      "Pafuth the he, q y fooul aieadpea er\n",
      "FWhimyod st f thaishe, ce hind hee'PNoucerld, s t hede, ngis;ed a l'sw ly t, fomindore ootha nt,\n",
      "T:\n",
      "we,\n",
      "Gerevet{Thelithe illd\n",
      "Meald m yold\n",
      ":\n",
      "-wa wo'therendt d maropemig I ungfas ns thay I d l:\n",
      "I I forinot ssichy tole s iburst hee'doo busth f wh tho r wstingghendere aromems ltore th orend mer ayon hie hy arer t mamatot me\n",
      "e de altanthillerthost l so de lire oushid Wy ave tay, sne Pour hes weakeveviathael n, seerd ulor our wo, be q-be, t sthad. t! ndepro whawe ouy sfondu meesonghigir bechedy my me we, thango harlor ds Heako y mied t I bur'd KEn'eusth, fprers t\n",
      "Mak atemy ald\n",
      "Bus, Pe ge how.\n",
      "\n",
      "G:\n",
      "\n",
      "\n",
      "\n",
      "Wh y Gedeea-it credo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model, \"The quick brown fox jumps over the lazy dog\", 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
