{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHeadConfig(typing.TypedDict):\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: AttentionHeadConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # linear layers to project the input to the key, query and value vectors\n",
    "\n",
    "        self.q = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        self.k = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        self.v = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.ctx_len, config.ctx_len)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        # q in (batch_size, ctx_len, d_k)\n",
    "        q = self.q(x)\n",
    "        # k in (batch_size, ctx_len, d_k)\n",
    "        k = self.k(x)\n",
    "\n",
    "        # masked self attention\n",
    "        # a in (batch_size, ctx_len, ctx_len)\n",
    "        a = (q @ k.transpose(-2, -1)) / (self.config.d_k ** 0.5)\n",
    "        a = a.masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "        a = F.softmax(a, dim=-1)\n",
    "        \n",
    "        # v in (batch_size, ctx_len, d_k)\n",
    "        v = self.v(x)\n",
    "        \n",
    "        # att in (batch_size, ctx_len, d_k)        \n",
    "        att = a @ v\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionConfig(typing.TypedDict):\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Dimension of the output vector\n",
    "    d_out: int\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: MultiHeadAttentionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(AttentionHeadConfig(\n",
    "                d_embed=config.d_embed,\n",
    "                d_k=config.d_k,\n",
    "                ctx_len=config.ctx_len\n",
    "            )) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.o = nn.Linear(config.n_heads*config.d_k, config.d_out, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.o(torch.cat([head(x) for head in self.heads], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockConfig(typing.TypedDict):\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Width of the feed-forward network\n",
    "    ff_width: int\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: TransformerBlockConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.ln1 = nn.LayerNorm(config.d_embed)\n",
    "        self.attn = MultiHeadAttention(MultiHeadAttentionConfig(\n",
    "            d_embed=config.d_embed,\n",
    "            d_k=config.d_k,\n",
    "            ctx_len=config.ctx_len,\n",
    "            n_heads=config.n_heads,\n",
    "            d_out=config.d_embed\n",
    "        ))\n",
    "        self.ln2 = nn.LayerNorm(config.d_embed)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.d_embed, config.ff_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.ff_width, config.d_embed)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConfig(typing.TypedDict):\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Width of the feed-forward network\n",
    "    ff_width: int\n",
    "    # Number of transformer blocks\n",
    "    n_blocks: int\n",
    "    # Number of tokens in the vocabulary\n",
    "    vocab_size: int\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_embed)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(TransformerBlockConfig(\n",
    "                d_embed=config.d_embed,\n",
    "                d_k=config.d_k,\n",
    "                ctx_len=config.ctx_len,\n",
    "                n_heads=config.n_heads,\n",
    "                ff_width=config.ff_width\n",
    "            )) for _ in range(config.n_blocks)\n",
    "        ])\n",
    "        self.unembed = nn.Linear(config.d_embed, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x in (batch_size, ctx_len)\n",
    "        x = self.embed(x)\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        x = self.unembed(x)\n",
    "        # x in (batch_size, ctx_len, vocab_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
