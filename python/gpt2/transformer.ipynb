{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHeadConfig(typing.NamedDict):\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: AttentionHeadConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # linear layers to project the input to the key, query and value vectors\n",
    "\n",
    "        self.q = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        self.k = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        self.v = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.ctx_len, config.ctx_len)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        # q in (batch_size, ctx_len, d_k)\n",
    "        q = self.q(x)\n",
    "        # k in (batch_size, ctx_len, d_k)\n",
    "        k = self.k(x)\n",
    "\n",
    "        # masked self attention\n",
    "        # a in (batch_size, ctx_len, ctx_len)\n",
    "        a = (q @ k.transpose(-2, -1)) / (self.config.d_k ** 0.5)\n",
    "        a = a.masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "        a = F.softmax(a, dim=-1)\n",
    "        \n",
    "        # v in (batch_size, ctx_len, d_k)\n",
    "        v = self.v(x)\n",
    "        \n",
    "        # att in (batch_size, ctx_len, d_k)        \n",
    "        att = a @ v\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionConfig(typing.NamedDict):\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Dimension of the output vector\n",
    "    d_out: int\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: MultiHeadAttentionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(AttentionHeadConfig(\n",
    "                d_embed=config.d_embed,\n",
    "                d_k=config.d_k,\n",
    "                ctx_len=config.ctx_len\n",
    "            )) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.o = nn.Linear(config.n_heads*config.d_k, config.d_out, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.o(torch.cat([head(x) for head in self.heads], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockConfig(typing.NamedDict):\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Number of layers in the feed-forward network\n",
    "    n_ff_layers: int\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: TransformerBlockConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attn = MultiHeadAttention(MultiHeadAttentionConfig(\n",
    "            d_embed=config.d_embed,\n",
    "            d_k=config.d_k,\n",
    "            ctx_len=config.ctx_len,\n",
    "            n_heads=config.n_heads,\n",
    "            d_out=config.d_embed\n",
    "        ))\n",
    "        self.ln1 = nn.LayerNorm(config.d_embed)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.d_embed, config.d_embed * config.n_ff_layers),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.d_embed * config.n_ff_layers, config.d_embed)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(config.d_embed)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        # att in (batch_size, ctx_len, d_embed)\n",
    "        att = self.attn(x)\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        x = self.ln1(x + self.dropout(att))\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        x = self.ln2(x + self.dropout(self.ff(x)))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
