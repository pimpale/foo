{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import typing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHeadConfig(typing.TypedDict):\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: AttentionHeadConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # linear layers to project the input to the key, query and value vectors\n",
    "\n",
    "        self.q = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        self.k = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        self.v = nn.Linear(config.d_embed, config.d_k, bias=False)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.ctx_len, config.ctx_len)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        # q in (batch_size, ctx_len, d_k)\n",
    "        q = self.q(x)\n",
    "        # k in (batch_size, ctx_len, d_k)\n",
    "        k = self.k(x)\n",
    "\n",
    "        # masked self attention\n",
    "        # a in (batch_size, ctx_len, ctx_len)\n",
    "        a = (q @ k.transpose(-2, -1)) / (self.config.d_k ** 0.5)\n",
    "        a = a.masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "        a = F.softmax(a, dim=-1)\n",
    "        \n",
    "        # v in (batch_size, ctx_len, d_k)\n",
    "        v = self.v(x)\n",
    "        \n",
    "        # att in (batch_size, ctx_len, d_k)        \n",
    "        att = a @ v\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionConfig(typing.TypedDict):\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Dimension of the output vector\n",
    "    d_out: int\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: MultiHeadAttentionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(AttentionHeadConfig(\n",
    "                d_embed=config.d_embed,\n",
    "                d_k=config.d_k,\n",
    "                ctx_len=config.ctx_len\n",
    "            )) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.o = nn.Linear(config.n_heads*config.d_k, config.d_out, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.o(torch.cat([head(x) for head in self.heads], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockConfig(typing.TypedDict):\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Width of the feed-forward network\n",
    "    ff_width: int\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: TransformerBlockConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.ln1 = nn.LayerNorm(config.d_embed)\n",
    "        self.attn = MultiHeadAttention(MultiHeadAttentionConfig(\n",
    "            d_embed=config.d_embed,\n",
    "            d_k=config.d_k,\n",
    "            ctx_len=config.ctx_len,\n",
    "            n_heads=config.n_heads,\n",
    "            d_out=config.d_embed\n",
    "        ))\n",
    "        self.ln2 = nn.LayerNorm(config.d_embed)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.d_embed, config.ff_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.ff_width, config.d_embed)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConfig(typing.TypedDict):\n",
    "    # Dimension of the embedding of each token\n",
    "    d_embed: int \n",
    "    # Dimension of the key, query and value vectors\n",
    "    d_k: int\n",
    "    # Size of the input sequence\n",
    "    ctx_len: int\n",
    "    # Number of attention heads\n",
    "    n_heads: int\n",
    "    # Width of the feed-forward network\n",
    "    ff_width: int\n",
    "    # Number of transformer blocks\n",
    "    n_blocks: int\n",
    "    # Number of tokens in the vocabulary\n",
    "    vocab_size: int\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_embed)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(TransformerBlockConfig(\n",
    "                d_embed=config.d_embed,\n",
    "                d_k=config.d_k,\n",
    "                ctx_len=config.ctx_len,\n",
    "                n_heads=config.n_heads,\n",
    "                ff_width=config.ff_width\n",
    "            )) for _ in range(config.n_blocks)\n",
    "        ])\n",
    "        self.unembed = nn.Linear(config.d_embed, config.vocab_size)\n",
    "\n",
    "    # one of the reasons that transformers are so popular is that they give you ctx_len tokens in parallel\n",
    "    # they predict the probability distribution over the next token for each of the ctx_len tokens\n",
    "    # this is why the input is (batch_size, ctx_len) and the output is (batch_size, ctx_len, vocab_size)\n",
    "    def forward(self, x):\n",
    "        # x in (batch_size, ctx_len)\n",
    "        x = self.embed(x)\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # x in (batch_size, ctx_len, d_embed)\n",
    "        x = self.unembed(x)\n",
    "        # x in (batch_size, ctx_len, vocab_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "chars = \" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "vocab = defaultdict(lambda: 1)\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<unk>\"] = 1\n",
    "for i, c in enumerate(chars):\n",
    "    vocab[c] = i + 2\n",
    "\n",
    "model_config = TransformerConfig(\n",
    "    d_embed=256,\n",
    "    d_k=64,\n",
    "    ctx_len=128,\n",
    "    n_heads=8,\n",
    "    ff_width=1024,\n",
    "    n_blocks=6,\n",
    "    vocab_size=len(vocab)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Transformer(model_config).to(device)\n",
    "model.compile()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text:str):\n",
    "    return torch.tensor([vocab[c] for c in text], dtype=torch.long)\n",
    "\n",
    "\n",
    "def prep_training_data(dataset:list[str], ctx_len):\n",
    "    # tokenize each data source:\n",
    "    X = []\n",
    "    Y = []\n",
    "    for text in dataset:\n",
    "        # tokenize\n",
    "        tokenized_text = tokenize(text)\n",
    "        if len(tokenized_text) >= ctx_len + 1:\n",
    "            # for each sequence, the first ctx_len tokens are the input, the last token is the output\n",
    "            X.extend(tokenized_text[:-1].unfold(0, ctx_len, 1))\n",
    "            Y.extend(tokenized_text[1:].unfold(0, ctx_len, 1))\n",
    "        elif len(tokenized_text) >= 1:\n",
    "            # remove last token\n",
    "            truncated_tokenized_text = tokenized_text[:-1]\n",
    "            # pad to ctx_len\n",
    "            padded_truncated_tokenized_text = F.pad(truncated_tokenized_text, (0, ctx_len - len(truncated_tokenized_text)), value=vocab[\"<pad>\"])\n",
    "            X.append(padded_truncated_tokenized_text)\n",
    "            # last token is the output\n",
    "            Y_final.append(tokenized_text[-1])\n",
    "    # X_tensor in (n_sequences, ctx_len)\n",
    "    X_tensor = torch.stack(X)\n",
    "    # Y_tensor in (n_sequences, ctx_len)\n",
    "    Y_tensor = torch.stack(Y)\n",
    "    return X_tensor, Y_final_tensor\n",
    "\n",
    "\n",
    "def train(model:Transformer, optimizer:torch.optim.Optimizer, dataloader:DataLoader):\n",
    "    model.train()\n",
    "    for X, Y_final in dataloader:\n",
    "        X = X.to(device)\n",
    "        Y_final = Y_final.to(device)\n",
    "        Y_pred = model(X)\n",
    "        # we n\n",
    "        loss = F.cross_entropy(Y_pred.view(-1, Y_pred.shape[-1]), Y_final.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
