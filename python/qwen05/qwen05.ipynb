{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2ForCausalLM\n",
    "from transformers import Qwen2Tokenizer\n",
    "device = \"cuda:2\" # the device to load the model onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8950387e253f43ddb1de491471342fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "# Now you do not need to add \"trust_remote_code=True\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    ").to(device)\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instead of using model.chat(), we directly use model.generate()\n",
    "# But you need to use tokenizer.apply_chat_template() to format your inputs as shown below\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Directly use generate() and tokenizer.decode() to get the output.\n",
    "# Use `max_new_tokens` to control the maximum output length.\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A Large Language Model (LLM) refers to a type of artificial intelligence (AI) that has been trained on vast amounts of text data, allowing it to generate human-like responses and perform various natural language processing tasks. These models are characterized by their ability to understand context, generate coherent sentences, and engage in conversations.\\n\\nThe training process for an LLM involves feeding it a massive dataset of text, such as books, articles, web pages, or any other written content. The AI then learns patterns, structures, and relationships within the language, enabling it to predict the next word or sequence of words in a sentence based on what it has learned from the training data.\\n\\nLLMs have several applications, including:\\n\\n1. **Language Translation**: LLMs can translate text from one language to another, maintaining the meaning and context of the original text.\\n2. **Text Generation**: They can create new text, such as articles, stories, or summaries, based on given prompts or inputs.\\n3. **Dialogue Systems**: LLMs can be used to build chatbots or conversational agents that can engage in natural conversations with users.\\n4. **Content Recommendation**: They can analyze user preferences and suggest relevant content, like articles or products.\\n5. **Code Generation**: Some LLMs can also generate code snippets based on descriptions or specifications provided by users.\\n\\nNotable examples of large language models include Google's BERT (Bidirectional Encoder Representations from Transformers), Alibaba's DAMO ERNIE, and Microsoft's Qwen. As these models continue to grow in size and complexity, they are becoming more sophisticated and capable, offering increasingly accurate and nuanced language processing capabilities.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
