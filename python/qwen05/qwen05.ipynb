{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import Qwen2ForCausalLM\n",
    "from transformers import Qwen2Tokenizer\n",
    "device = \"cuda:2\" # the device to load the model onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "# model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    ").to(device)\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm()\n",
      "        (post_attention_layernorm): Qwen2RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary: generates the next token given the input token ids\n",
    "# inputs: \n",
    "#   model: the model to generate the next token\n",
    "#   input_ids: the input token ids: tensor of shape (batch_size, sequence_length)\n",
    "# outputs: a tensor of shape (batch_size, 1) containing the generated token id\n",
    "def generate_next_token(model, input_ids):\n",
    "    assert input_ids.dim() == 1\n",
    "    outputs = model.forward(input_ids.unsqueeze(0))\n",
    "    logits = outputs.logits[0]\n",
    "    next_token_logits = logits[-1, :]\n",
    "    next_token_id = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "    return next_token_id\n",
    "\n",
    "\n",
    "# summary: generates the next N tokens given the input token ids\n",
    "# inputs:\n",
    "#   model: the model to generate the next token\n",
    "#   input_ids: the input token ids: tensor of shape (sequence_length)\n",
    "#   num_tokens: the number of tokens to generate\n",
    "# outputs: a tensor of shape (num_tokens) containing the generated token ids\n",
    "def generate_next_tokens(model, input_ids, num_tokens):\n",
    "    assert input_ids.dim() == 1\n",
    "    for i in range(num_tokens):\n",
    "        next_token_id = generate_next_token(model, input_ids)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "    return input_ids\n",
    "\n",
    "# summary: generates text given a prompt\n",
    "# inputs:\n",
    "#   model: the model to generate the next token\n",
    "#   tokenizer: the tokenizer to convert text to token ids\n",
    "#   prompt: the prompt text\n",
    "#   num_tokens: the number of tokens to generate\n",
    "# outputs: a list of strings containing the generated text\n",
    "def generate_text(model, tokenizer, prompt, num_tokens):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")[0].to(device)\n",
    "    output_ids = generate_next_tokens(model, input_ids, num_tokens)\n",
    "    output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is to know that life is a journey full of curiosity and exploration but saddling ones foot into a tight negotiation and realize that life is also a weapon.\\nIn organic garden and vegetables, when children are not looking he or she lies under the tree and calculus gives them some flowers or leaves and the parents take the plant back and say “oh maybe another one is growing ? Oh just one other? But let’s look at it another way?\\nWhen are you born ?\\n5.rules to be kept in the ocean'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, \"The meaning of life is\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 785, 7290,  315, 2272,  374], device='cuda:2')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"The meaning of life is\", return_tensors=\"pt\")[0].to(device)\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([537], device='cuda:2')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_t = generate_next_token(model, input_ids)\n",
    "\n",
    "next_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = torch.cat([input_ids, next_t], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is not'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.cat([input_ids, next_t], dim=-1), skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
