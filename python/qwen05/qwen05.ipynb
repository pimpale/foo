{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import Qwen2ForCausalLM\n",
    "from transformers import Qwen2Tokenizer\n",
    "device = \"cuda:2\" # the device to load the model onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22b385fc6354e50aaefa050afe8b7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"Qwen/Qwen2-0.5B\"\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "# Now you do not need to add \"trust_remote_code=True\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    ").to(device)\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary: generates the next token given the input token ids\n",
    "# inputs: \n",
    "#   model: the model to generate the next token\n",
    "#   input_ids: the input token ids: tensor of shape (batch_size, sequence_length)\n",
    "# outputs: a tensor of shape (batch_size, 1) containing the generated token id\n",
    "def generate_next_token(model, input_ids):\n",
    "    assert input_ids.dim() == 1\n",
    "    outputs = model.forward(input_ids.unsqueeze(0))\n",
    "    logits = outputs.logits[0]\n",
    "    next_token_logits = logits[-1, :]\n",
    "    next_token_id = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "    return next_token_id\n",
    "\n",
    "\n",
    "# summary: generates the next N tokens given the input token ids\n",
    "# inputs:\n",
    "#   model: the model to generate the next token\n",
    "#   input_ids: the input token ids: tensor of shape (sequence_length)\n",
    "#   num_tokens: the number of tokens to generate\n",
    "# outputs: a tensor of shape (num_tokens) containing the generated token ids\n",
    "def generate_next_tokens(model, input_ids, num_tokens):\n",
    "    assert input_ids.dim() == 1\n",
    "    for i in range(num_tokens):\n",
    "        next_token_id = generate_next_token(model, input_ids)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "    return input_ids\n",
    "\n",
    "# summary: generates text given a prompt\n",
    "# inputs:\n",
    "#   model: the model to generate the next token\n",
    "#   tokenizer: the tokenizer to convert text to token ids\n",
    "#   prompt: the prompt text\n",
    "#   num_tokens: the number of tokens to generate\n",
    "# outputs: a list of strings containing the generated text\n",
    "def generate_text(model, tokenizer, prompt, num_tokens):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")[0].to(device)\n",
    "    output_ids = generate_next_tokens(model, input_ids, num_tokens)\n",
    "    output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The meaning of life is a fundamental question that has puzzled philosophers, scientists, and individuals throughout history. While many interpretations exist, the meaning of life is deeply personal and subjective, and often varies based on culture, religion, and individual beliefs. So, what is the meaning of life? Here are some of the common interpretations:\\n1. Survival: One common interpretation is that the meaning of life is to survive and thrive. This can mean providing for oneself and loved ones, building relationships, and experiencing personal growth and development.\\n2']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, \"The meaning of life is\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 785, 7290,  315, 2272,  374], device='cuda:2')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"The meaning of life is\", return_tensors=\"pt\")[0].to(device)\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([429], device='cuda:2')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_t = generate_next_token(model, input_ids)\n",
    "\n",
    "next_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 785, 7290,  315, 2272,  374,  429], device='cuda:2')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = torch.cat([input_ids, next_t], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is that'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.cat([input_ids, next_t], dim=-1), skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
